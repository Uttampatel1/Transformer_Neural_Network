{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention in Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Generate Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "L, d_k, d_v = 4, 10, 10\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 2.1242154  -0.05097662 -0.41177697  1.2434784   0.21638206  0.40670121\n",
      "   1.37134177  0.13827728 -0.24159657  1.31941207]\n",
      " [-1.65945225  2.5133612  -0.52408406  0.52430126  1.29746741 -0.24841566\n",
      "  -1.35109741 -1.50804762 -0.69676327  0.17439543]\n",
      " [ 0.74734326  0.01635423 -1.81193073  0.48258882  0.02638898 -1.00850241\n",
      "  -2.42754604  0.64763816 -1.17444421 -0.41234463]\n",
      " [-1.02032181 -0.58109091 -0.97416148 -1.13679723 -1.2481006   0.52204578\n",
      "   0.00593693 -0.27470854  0.70069493  1.94716725]]\n",
      "K\n",
      " [[-0.06408409 -1.26976691  0.5749145  -0.10629726  2.16337022 -0.38491943\n",
      "  -0.41933464 -1.04573974  1.02006385 -0.14139777]\n",
      " [ 1.63077553  0.10346948  1.91457149 -0.64366476 -0.18445495 -0.66989645\n",
      "  -1.27784937  0.11644065  1.12501153  0.43821596]\n",
      " [-1.8321942   0.42343224 -0.24660732 -0.76167373 -1.19830539  0.53104853\n",
      "   2.13644932 -0.84111169  0.3407909  -1.37863616]\n",
      " [-0.20837821 -1.51194795 -1.24852003 -1.36934492 -0.40535539  1.16545723\n",
      "   0.76523327  1.04657534  1.68085271  1.14996351]]\n",
      "V\n",
      " [[-8.71405964e-01 -4.07135166e-02  2.41630292e+00 -4.19370604e-01\n",
      "   2.35233482e+00 -2.22227973e-01 -2.14203221e+00 -1.98854107e-01\n",
      "  -1.86826970e-01  1.00402161e+00]\n",
      " [ 4.58524346e-01  1.17251196e+00  5.67021406e-02  1.18419210e+00\n",
      "   9.06955998e-01 -1.31980687e+00 -6.41618132e-01  3.02434683e-01\n",
      "  -1.97652208e-01  9.82027039e-01]\n",
      " [-5.58676086e-02  9.68400703e-01 -6.91099233e-01  1.28634942e-03\n",
      "  -1.60245819e+00  6.30187748e-01  2.85196222e-01 -5.09184335e-01\n",
      "  -6.01675745e-01  1.70758649e+00]\n",
      " [-1.17919071e+00  2.66415804e+00 -6.57343739e-01 -9.80253645e-01\n",
      "  -1.68417141e+00 -1.52338139e+00 -1.82703734e+00  1.05314611e+00\n",
      "  -9.42450930e-02 -2.67502422e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "$$\n",
    "\\text{self attention} = softmax\\bigg(\\frac{Q.K^T}{\\sqrt{d_k}}+M\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{new V} = \\text{self attention}.V\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.55199318,  0.97288475, -0.2141719 , -0.74254354],\n",
       "       [-1.35526289,  2.62442942,  0.35446297, -2.05105865],\n",
       "       [ 1.63129485,  3.14958207,  0.46746191, -1.3425128 ],\n",
       "       [-1.60039187,  1.77116277, -1.17169757,  1.20568295]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9001736505696285, 1.3136031103635024, 3.146410416897136)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why we need sqrt(d_k) in denominator\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.21872256513619, 1.1430141103758429, 1.3286904104194095)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notice the reduction in variance of the product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40521635,  0.04042821, -1.23021428,  0.35967032],\n",
       "       [ 0.27468612, -0.95388936,  0.01641138, -2.50330607],\n",
       "       [-0.4792087 , -0.06736921, -2.34423138, -1.07302556],\n",
       "       [-0.57326447, -0.43479068,  0.72745546,  2.56537949]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking\n",
    "\n",
    "    - This is to ensure words don't get context from words generated in the future.\n",
    "    - Not required in the encoders, but required int he decoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones( (L, L) ))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40521635,        -inf,        -inf,        -inf],\n",
       "       [ 0.27468612, -0.95388936,        -inf,        -inf],\n",
       "       [-0.4792087 , -0.06736921, -2.34423138,        -inf],\n",
       "       [-0.57326447, -0.43479068,  0.72745546,  2.56537949]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "$$\n",
    "\\text{softmax} = \\frac{e^{x_i}}{\\sum_j e^x_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19423197, 0.30329251, 0.08511942, 0.4173561 ],\n",
       "       [0.47008929, 0.13759948, 0.36308898, 0.02922225],\n",
       "       [0.31087758, 0.46929833, 0.04815267, 0.17167142],\n",
       "       [0.03461044, 0.03975077, 0.12708743, 0.79855137]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.77356915, 0.22643085, 0.        , 0.        ],\n",
       "       [0.37530708, 0.5665606 , 0.05813233, 0.        ],\n",
       "       [0.03461044, 0.03975077, 0.12708743, 0.79855137]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.87140596, -0.04071352,  2.41630292, -0.4193706 ,  2.35233482,\n",
       "        -0.22222797, -2.14203221, -0.19885411, -0.18682697,  1.00402161],\n",
       "       [-0.57026872,  0.23399815,  1.88201651, -0.05627454,  2.02505647,\n",
       "        -0.47075369, -1.80229218, -0.08534686, -0.18927814,  0.99904136],\n",
       "       [-0.07051071,  0.70531439,  0.89880557,  0.51359861,  1.30353882,\n",
       "        -0.79452002, -1.15085628,  0.06711615, -0.21707625,  1.03246021],\n",
       "       [-0.96067747,  2.29573774, -0.52686951, -0.75006147, -1.43108213,\n",
       "        -1.19656409, -1.52237979,  0.78141992, -0.16604796,  0.07718431]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.71405964e-01, -4.07135166e-02,  2.41630292e+00,\n",
       "        -4.19370604e-01,  2.35233482e+00, -2.22227973e-01,\n",
       "        -2.14203221e+00, -1.98854107e-01, -1.86826970e-01,\n",
       "         1.00402161e+00],\n",
       "       [ 4.58524346e-01,  1.17251196e+00,  5.67021406e-02,\n",
       "         1.18419210e+00,  9.06955998e-01, -1.31980687e+00,\n",
       "        -6.41618132e-01,  3.02434683e-01, -1.97652208e-01,\n",
       "         9.82027039e-01],\n",
       "       [-5.58676086e-02,  9.68400703e-01, -6.91099233e-01,\n",
       "         1.28634942e-03, -1.60245819e+00,  6.30187748e-01,\n",
       "         2.85196222e-01, -5.09184335e-01, -6.01675745e-01,\n",
       "         1.70758649e+00],\n",
       "       [-1.17919071e+00,  2.66415804e+00, -6.57343739e-01,\n",
       "        -9.80253645e-01, -1.68417141e+00, -1.52338139e+00,\n",
       "        -1.82703734e+00,  1.05314611e+00, -9.42450930e-02,\n",
       "        -2.67502422e-01]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "  return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  d_k = q.shape[-1]\n",
    "  scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scaled = scaled + mask\n",
    "  attention = softmax(scaled)\n",
    "  out = np.matmul(attention, v)\n",
    "  return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 2.1242154  -0.05097662 -0.41177697  1.2434784   0.21638206  0.40670121\n",
      "   1.37134177  0.13827728 -0.24159657  1.31941207]\n",
      " [-1.65945225  2.5133612  -0.52408406  0.52430126  1.29746741 -0.24841566\n",
      "  -1.35109741 -1.50804762 -0.69676327  0.17439543]\n",
      " [ 0.74734326  0.01635423 -1.81193073  0.48258882  0.02638898 -1.00850241\n",
      "  -2.42754604  0.64763816 -1.17444421 -0.41234463]\n",
      " [-1.02032181 -0.58109091 -0.97416148 -1.13679723 -1.2481006   0.52204578\n",
      "   0.00593693 -0.27470854  0.70069493  1.94716725]]\n",
      "K\n",
      " [[-0.06408409 -1.26976691  0.5749145  -0.10629726  2.16337022 -0.38491943\n",
      "  -0.41933464 -1.04573974  1.02006385 -0.14139777]\n",
      " [ 1.63077553  0.10346948  1.91457149 -0.64366476 -0.18445495 -0.66989645\n",
      "  -1.27784937  0.11644065  1.12501153  0.43821596]\n",
      " [-1.8321942   0.42343224 -0.24660732 -0.76167373 -1.19830539  0.53104853\n",
      "   2.13644932 -0.84111169  0.3407909  -1.37863616]\n",
      " [-0.20837821 -1.51194795 -1.24852003 -1.36934492 -0.40535539  1.16545723\n",
      "   0.76523327  1.04657534  1.68085271  1.14996351]]\n",
      "V\n",
      " [[-8.71405964e-01 -4.07135166e-02  2.41630292e+00 -4.19370604e-01\n",
      "   2.35233482e+00 -2.22227973e-01 -2.14203221e+00 -1.98854107e-01\n",
      "  -1.86826970e-01  1.00402161e+00]\n",
      " [ 4.58524346e-01  1.17251196e+00  5.67021406e-02  1.18419210e+00\n",
      "   9.06955998e-01 -1.31980687e+00 -6.41618132e-01  3.02434683e-01\n",
      "  -1.97652208e-01  9.82027039e-01]\n",
      " [-5.58676086e-02  9.68400703e-01 -6.91099233e-01  1.28634942e-03\n",
      "  -1.60245819e+00  6.30187748e-01  2.85196222e-01 -5.09184335e-01\n",
      "  -6.01675745e-01  1.70758649e+00]\n",
      " [-1.17919071e+00  2.66415804e+00 -6.57343739e-01 -9.80253645e-01\n",
      "  -1.68417141e+00 -1.52338139e+00 -1.82703734e+00  1.05314611e+00\n",
      "  -9.42450930e-02 -2.67502422e-01]]\n",
      "New V\n",
      " [[-0.52708575  1.54203855  0.15334822 -0.13130393 -0.10732795 -1.02560262\n",
      "  -1.34889855  0.44929783 -0.18678226  0.52655954]\n",
      " [-0.40128942  0.57166637  0.87354073 -0.06237557  0.5995539  -0.10177414\n",
      "  -1.04507126 -0.20596825 -0.33623808  1.21929502]\n",
      " [-0.26083938  1.04159187  0.6316592   0.25714686  0.79063434 -0.91964469\n",
      "  -1.26693724  0.23638932 -0.19598965  0.80929378]\n",
      " [-0.96067747  2.29573774 -0.52686951 -0.75006147 -1.43108213 -1.19656409\n",
      "  -1.52237979  0.78141992 -0.16604796  0.07718431]]\n",
      "Attention\n",
      " [[0.19423197 0.30329251 0.08511942 0.4173561 ]\n",
      " [0.47008929 0.13759948 0.36308898 0.02922225]\n",
      " [0.31087758 0.46929833 0.04815267 0.17167142]\n",
      " [0.03461044 0.03975077 0.12708743 0.79855137]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 2.1242154  -0.05097662 -0.41177697  1.2434784   0.21638206  0.40670121\n",
      "   1.37134177  0.13827728 -0.24159657  1.31941207]\n",
      " [-1.65945225  2.5133612  -0.52408406  0.52430126  1.29746741 -0.24841566\n",
      "  -1.35109741 -1.50804762 -0.69676327  0.17439543]\n",
      " [ 0.74734326  0.01635423 -1.81193073  0.48258882  0.02638898 -1.00850241\n",
      "  -2.42754604  0.64763816 -1.17444421 -0.41234463]\n",
      " [-1.02032181 -0.58109091 -0.97416148 -1.13679723 -1.2481006   0.52204578\n",
      "   0.00593693 -0.27470854  0.70069493  1.94716725]]\n",
      "K\n",
      " [[-0.06408409 -1.26976691  0.5749145  -0.10629726  2.16337022 -0.38491943\n",
      "  -0.41933464 -1.04573974  1.02006385 -0.14139777]\n",
      " [ 1.63077553  0.10346948  1.91457149 -0.64366476 -0.18445495 -0.66989645\n",
      "  -1.27784937  0.11644065  1.12501153  0.43821596]\n",
      " [-1.8321942   0.42343224 -0.24660732 -0.76167373 -1.19830539  0.53104853\n",
      "   2.13644932 -0.84111169  0.3407909  -1.37863616]\n",
      " [-0.20837821 -1.51194795 -1.24852003 -1.36934492 -0.40535539  1.16545723\n",
      "   0.76523327  1.04657534  1.68085271  1.14996351]]\n",
      "V\n",
      " [[-8.71405964e-01 -4.07135166e-02  2.41630292e+00 -4.19370604e-01\n",
      "   2.35233482e+00 -2.22227973e-01 -2.14203221e+00 -1.98854107e-01\n",
      "  -1.86826970e-01  1.00402161e+00]\n",
      " [ 4.58524346e-01  1.17251196e+00  5.67021406e-02  1.18419210e+00\n",
      "   9.06955998e-01 -1.31980687e+00 -6.41618132e-01  3.02434683e-01\n",
      "  -1.97652208e-01  9.82027039e-01]\n",
      " [-5.58676086e-02  9.68400703e-01 -6.91099233e-01  1.28634942e-03\n",
      "  -1.60245819e+00  6.30187748e-01  2.85196222e-01 -5.09184335e-01\n",
      "  -6.01675745e-01  1.70758649e+00]\n",
      " [-1.17919071e+00  2.66415804e+00 -6.57343739e-01 -9.80253645e-01\n",
      "  -1.68417141e+00 -1.52338139e+00 -1.82703734e+00  1.05314611e+00\n",
      "  -9.42450930e-02 -2.67502422e-01]]\n",
      "New V\n",
      " [[-0.87140596 -0.04071352  2.41630292 -0.4193706   2.35233482 -0.22222797\n",
      "  -2.14203221 -0.19885411 -0.18682697  1.00402161]\n",
      " [-0.57026872  0.23399815  1.88201651 -0.05627454  2.02505647 -0.47075369\n",
      "  -1.80229218 -0.08534686 -0.18927814  0.99904136]\n",
      " [-0.07051071  0.70531439  0.89880557  0.51359861  1.30353882 -0.79452002\n",
      "  -1.15085628  0.06711615 -0.21707625  1.03246021]\n",
      " [-0.96067747  2.29573774 -0.52686951 -0.75006147 -1.43108213 -1.19656409\n",
      "  -1.52237979  0.78141992 -0.16604796  0.07718431]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.77356915 0.22643085 0.         0.        ]\n",
      " [0.37530708 0.5665606  0.05813233 0.        ]\n",
      " [0.03461044 0.03975077 0.12708743 0.79855137]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
