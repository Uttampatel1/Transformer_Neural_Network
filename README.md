
## Transformer Cheat Sheet

#### Additional Resources:

- ğŸ“‘ [Notion Document with Further Details](https://www.notion.so/uttam-patel/Transformer-98a0f35922024b19896403ed85dc76a1)
- ğŸ¥ [Video Tutorial on Transformers](https://www.youtube.com/watch?v=zxQyTK8quyY&t=1890s)


#### Transformer Architecture:

1. **Self-Attention Mechanism:**
   - ğŸ”‘ Key to transformer's success.
   - ğŸ”„ Allows the model to weigh input tokens differently when making predictions for a given token.

2. **Multi-Head Attention:**
   - â¡ï¸ Multiple attention heads run in parallel.
   - ğŸ¯ Enables the model to focus on different aspects of the input sequence.

3. **Positional Encoding:**
   - ğŸ“ Addresses the lack of sequential information in self-attention.
   - â• Adds positional information to input embeddings.

4. **Feedforward Neural Network:**
   - â¡ï¸ Applies a linear transformation followed by a non-linear activation function (commonly ReLU).

5. **Layer Normalization:**
   - ğŸ“Š Normalizes inputs across the feature dimension.

6. **Encoder and Decoder Stacks:**
   - ğŸ”— Stacks of identical layers for both encoder and decoder.

![nn model](images/full-nn.png)
-----
![gif file](images/nne.gif)


#### Training:

1. **Loss Function:**
   - ğŸ“‰ Typically cross-entropy loss for classification tasks.
   - ğŸ“Š Mean squared error for regression tasks.

2. **Optimization:**
   - ğŸš€ Adam optimizer is commonly used.
   - ğŸ“ˆ Learning rate scheduling is crucial for stable training.

#### Model Parameters:

1. **Embedding Dimension:**
   - ğŸ“ Determines the dimensionality of input and output embeddings.

2. **Number of Encoder and Decoder Layers:**
   - ğŸ”¢ The depth of the model, usually in the range of 6 to 12 layers.

3. **Number of Attention Heads:**
   - ğŸ” Determines the number of parallel attention mechanisms in multi-head attention.

4. **Dropout:**
   - âŒ Regularization technique to prevent overfitting.
   - ğŸ”„ Applied to the output of each sub-layer.

#### Inference:

1. **Greedy Decoding:**
   - ğŸš¶â€â™‚ï¸ Selects the token with the highest probability at each step.

2. **Beam Search:**
   - ğŸ›¤ï¸ Expands the search space by considering multiple candidates at each step.

3. **Temperature Scaling:**
   - ğŸŒ¡ï¸ Adjusts the softmax temperature during sampling for controlling the diversity of generated sequences.

#### Applications:

1. **Natural Language Processing (NLP):**
   - ğŸ“° Sentiment analysis, text summarization, machine translation.

2. **Image Generation:**
   - ğŸ–¼ï¸ Transformer models adapted for image generation tasks.

3. **Speech Recognition:**
   - ğŸ—£ï¸ Transformers applied to convert spoken language into text.


#### Common Libraries:

1. **Transformers Library:**
   - ğŸ¤— Hugging Face's Transformers library is widely used for pre-trained models and convenient APIs.

2. **TensorFlow, PyTorch:**
   - ğŸ§  Popular deep learning frameworks for implementing and training transformer models.

#### Tips for Efficient Training:

1. **Batching:**
   - ğŸ² Use batch processing for faster training.
   - ğŸš§ Padding and masking are crucial for handling variable-length sequences.

2. **Mixed Precision Training:**
   - âš–ï¸ Utilize half-precision (float16) for faster training without sacrificing much precision.

3. **Model Parallelism:**
   - ğŸ”„ Distribute the model across multiple GPUs or devices for handling large models.


#### Popular Models:

1. **BERT (Bidirectional Encoder Representations from Transformers):**
   - ğŸ¤– Pre-trained on massive datasets for various NLP tasks.

2. **GPT (Generative Pre-trained Transformer):**
   - ğŸ—£ï¸ Known for generating coherent and contextually relevant text.

3. **T5 (Text-To-Text Transfer Transformer):**
   - ğŸ”„ Treats all NLP tasks as a text-to-text problem.


![Transformer_Architecture_complete](Transformer_Architecture_complete.png)
