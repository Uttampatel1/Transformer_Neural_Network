{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentence Tokenization**\n",
    "\n",
    "Sentence tokenization is the process of splitting a text into individual sentences. It is a crucial step in natural language processing, as many algorithms and models operate on a sentence-level basis. Tokenization refers to the process of breaking a large text into smaller chunks, called tokens. In the case of sentence tokenization, the text is broken into individual sentences, with each sentence being considered as a separate token.\n",
    "\n",
    "Sentence tokenization can be achieved using various techniques, such as using regular expressions to match patterns of punctuation marks, or using machine learning models that have been trained on large datasets. Once the text has been tokenized into individual sentences, it can then be further preprocessed and analyzed, such as by removing stop words, stemming, or lemmatization.\n",
    "\n",
    "Sentence tokenization is an important preprocessing step in many NLP tasks, such as machine translation, text summarization, sentiment analysis, and more. It allows for the text to be segmented into smaller, more manageable units, which can then be analyzed and processed more efficiently. Additionally, sentence tokenization helps to ensure that the output of an NLP model is coherent and meaningful, as it ensures that each output is a complete sentence that can be understood by a human reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing/Loading the File containing the English and its corresponding Gujarati Sentence¶\n",
    "dataset link: https://www.kaggle.com/datasets/parvmodi/english-to-gujarati-machine-translation-dataset/data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_file = '/workspaces/codespaces-blank/archive/en-gu/train.en'\n",
    "gujarati_file = '/workspaces/codespaces-blank/archive/en-gu/train.gu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating the Alpha Syllabery/Vocabulary for both Languages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<s>'\n",
    "PADDING_TOKEN = '</s>'\n",
    "END_TOKEN = '<pad>'\n",
    "\n",
    "gujarati_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
    "                       '૦', '૧', '૨', '૩', '૪', '૫', '૬', '૭', '૮', '૯', ':', '<', '=', '>', '?', '@',\n",
    "                       'અ', 'આ', 'ઇ', 'ઈ', 'ઉ', 'ઊ', 'ઋ', 'ૠ', 'ઌ', 'ૡ', 'ઍ', 'એ', 'ઐ', 'ઑ', 'ઓ', 'ઔ',\n",
    "                       'ક', 'ખ', 'ગ', 'ઘ', 'ઙ',\n",
    "                       'ચ', 'છ', 'જ', 'ઝ', 'ઞ',                        \n",
    "                       'ટ', 'ઠ', 'ડ', 'ઢ', 'ણ',                        \n",
    "                       'ત', 'થ', 'દ', 'ધ', 'ન',                        \n",
    "                       'પ', 'ફ', 'બ', 'ભ', 'મ',                        \n",
    "                       'ય', 'ર', 'લ', 'વ', 'શ', 'ષ', 'સ', 'હ', '઼', 'ા', 'િ', 'ી', 'ુ', 'ૂ', 'ૃ', 'ૄ', 'ૅ', 'ે', 'ૈ', 'ૉ', 'ો', 'ૌ', '્', 'ૐ', 'ૠ', 'ૡ', 'ં', 'ઃ',\n",
    "                       PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "\n",
    "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                        ':', '<', '=', '>', '?', '@', \n",
    "                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', \n",
    "                        'Y', 'Z',\n",
    "                        \"[\", \"/\", \"]\", \"^\", \"_\", \"`\", \n",
    "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "                        'y', 'z', \n",
    "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m', 'y']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'my'\n",
    "list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'myI'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'my' + 'I'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_gujarati = {k:v for k,v in enumerate(gujarati_vocabulary)}\n",
    "gujarati_to_index = {v:k for k,v in enumerate(gujarati_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '#': 4,\n",
       " '$': 5,\n",
       " '%': 6,\n",
       " '&': 7,\n",
       " \"'\": 8,\n",
       " '(': 9,\n",
       " ')': 10,\n",
       " '*': 11,\n",
       " '+': 12,\n",
       " ',': 13,\n",
       " '-': 14,\n",
       " '.': 15,\n",
       " '/': 16,\n",
       " '૦': 17,\n",
       " '૧': 18,\n",
       " '૨': 19,\n",
       " '૩': 20,\n",
       " '૪': 21,\n",
       " '૫': 22,\n",
       " '૬': 23,\n",
       " '૭': 24,\n",
       " '૮': 25,\n",
       " '૯': 26,\n",
       " ':': 27,\n",
       " '<': 28,\n",
       " '=': 29,\n",
       " '>': 30,\n",
       " '?': 31,\n",
       " '@': 32,\n",
       " 'અ': 33,\n",
       " 'આ': 34,\n",
       " 'ઇ': 35,\n",
       " 'ઈ': 36,\n",
       " 'ઉ': 37,\n",
       " 'ઊ': 38,\n",
       " 'ઋ': 39,\n",
       " 'ૠ': 98,\n",
       " 'ઌ': 41,\n",
       " 'ૡ': 99,\n",
       " 'ઍ': 43,\n",
       " 'એ': 44,\n",
       " 'ઐ': 45,\n",
       " 'ઑ': 46,\n",
       " 'ઓ': 47,\n",
       " 'ઔ': 48,\n",
       " 'ક': 49,\n",
       " 'ખ': 50,\n",
       " 'ગ': 51,\n",
       " 'ઘ': 52,\n",
       " 'ઙ': 53,\n",
       " 'ચ': 54,\n",
       " 'છ': 55,\n",
       " 'જ': 56,\n",
       " 'ઝ': 57,\n",
       " 'ઞ': 58,\n",
       " 'ટ': 59,\n",
       " 'ઠ': 60,\n",
       " 'ડ': 61,\n",
       " 'ઢ': 62,\n",
       " 'ણ': 63,\n",
       " 'ત': 64,\n",
       " 'થ': 65,\n",
       " 'દ': 66,\n",
       " 'ધ': 67,\n",
       " 'ન': 68,\n",
       " 'પ': 69,\n",
       " 'ફ': 70,\n",
       " 'બ': 71,\n",
       " 'ભ': 72,\n",
       " 'મ': 73,\n",
       " 'ય': 74,\n",
       " 'ર': 75,\n",
       " 'લ': 76,\n",
       " 'વ': 77,\n",
       " 'શ': 78,\n",
       " 'ષ': 79,\n",
       " 'સ': 80,\n",
       " 'હ': 81,\n",
       " '઼': 82,\n",
       " 'ા': 83,\n",
       " 'િ': 84,\n",
       " 'ી': 85,\n",
       " 'ુ': 86,\n",
       " 'ૂ': 87,\n",
       " 'ૃ': 88,\n",
       " 'ૄ': 89,\n",
       " 'ૅ': 90,\n",
       " 'ે': 91,\n",
       " 'ૈ': 92,\n",
       " 'ૉ': 93,\n",
       " 'ો': 94,\n",
       " 'ૌ': 95,\n",
       " '્': 96,\n",
       " 'ૐ': 97,\n",
       " 'ં': 100,\n",
       " 'ઃ': 101,\n",
       " '</s>': 102,\n",
       " '<pad>': 103}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gujarati_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Lines from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(english_file, 'r') as file:\n",
    "    english_sentences = file.readlines()\n",
    "with open(gujarati_file, 'r') as file:\n",
    "    gujarati_sentences = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit Number of sentences\n",
    "TOTAL_SENTENCES = 100000\n",
    "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
    "gujarati_sentences = gujarati_sentences[:TOTAL_SENTENCES]\n",
    "english_sentences = [sentence.rstrip('\\n') for sentence in english_sentences]\n",
    "gujarati_sentences = [sentence.rstrip('\\n') for sentence in gujarati_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ઓનલાઈન ટ્રાન્ઝેક્શન કરી શકાય?',\n",
       " 'કુરાન તે વર્ણવે છે:',\n",
       " 'એક પેસેન્જર ટ્રેન સ્ટેશન પર બેઠેલું છે.',\n",
       " 'ભારે બરફના ટૂકડાweather forecast']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gujarati_sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Are you doing online transactions?',\n",
       " 'Kunwar explains:',\n",
       " 'A passenger train is sitting at a station.',\n",
       " 'heavy snow shower']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1182, 888)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max(len(x) for x in gujarati_sentences), max(len(x) for x in english_sentences),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the top 99 Percentile of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99th percentile length Gujarati: 228.0\n",
      "99th percentile length English: 244.00999999999476\n"
     ]
    }
   ],
   "source": [
    "PERCENTILE = 99\n",
    "print( f\"{PERCENTILE}th percentile length Gujarati: {np.percentile([len(x) for x in gujarati_sentences], PERCENTILE)}\" )\n",
    "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences having vocab context and length of sentence should be upto the max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 100000\n",
      "Number of valid sentences: 69845\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 300\n",
    "\n",
    "def is_valid_tokens(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
    "\n",
    "valid_sentence_indicies = []\n",
    "for index in range(len(gujarati_sentences)):\n",
    "    gujarati_sentence, english_sentence = gujarati_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(gujarati_sentence, max_sequence_length) \\\n",
    "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
    "      and is_valid_tokens(gujarati_sentence, gujarati_vocabulary):\n",
    "        valid_sentence_indicies.append(index)\n",
    "\n",
    "print(f\"Number of sentences: {len(gujarati_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gujarati_sentences = [gujarati_sentences[i] for i in valid_sentence_indicies]\n",
    "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ઓનલાઈન ટ્રાન્ઝેક્શન કરી શકાય?',\n",
       " 'કુરાન તે વર્ણવે છે:',\n",
       " 'એક પેસેન્જર ટ્રેન સ્ટેશન પર બેઠેલું છે.',\n",
       " 'પાંચ મહિનાના કોર્સમાં પોતે જે કંઈ શીખ્યો, એ એક વિદ્યાર્થીએ પોતાના નાના ભાઈને જણાવ્યું.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gujarati_sentences[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Dataset out of the final choosen English and Gujarati Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, english_sentences, gujarati_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.gujarati_sentences = gujarati_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.gujarati_sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69845"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextDataset(english_sentences, gujarati_sentences)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('On the contrary, his righteous acts have provided a basis on which mankind can approach him and be saved from the dire consequences of sin.',\n",
       " 'આથી તે કંઈ કઠોર બની જતા નથી અથવા તેમનો ડર રાખીને તેમનાથી દૂર રહીએ એવા પણ બનતા નથી.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Are you doing online transactions?', 'Kunwar explains:', 'A passenger train is sitting at a station.'), ('ઓનલાઈન ટ્રાન્ઝેક્શન કરી શકાય?', 'કુરાન તે વર્ણવે છે:', 'એક પેસેન્જર ટ્રેન સ્ટેશન પર બેઠેલું છે.')]\n",
      "[('It was plain that their intensive study of the Scriptures over their five months of training had reached their heart and motivated them to share with others what they had learned.', 'Jesus Christ is overseeing the greatest preaching campaign in history', 'The Moskals immediately included the reading of the Harp book in their regular Bible - reading sessions.'), ('પાંચ મહિનાના કોર્સમાં પોતે જે કંઈ શીખ્યો, એ એક વિદ્યાર્થીએ પોતાના નાના ભાઈને જણાવ્યું.', 'આજે પૃથ્વી પર થઈ રહેલા મહાન પ્રચાર કાર્યની ઈસુ દેખરેખ રાખે છે', 'મૉસ્કેલ કુટુંબે બાઇબલ સાથે સાથે એ પુસ્તક પણ વાંચવાનું શરૂ કરી દીધું.')]\n",
      "[('Gas lasers.', 'Then the job.', 'Australia announce ODI squad for India series'), ('ગેસ લેસર્સ.', 'પછી તો કામ જ કામ છે.', 'ઓસ્ટ્રેલિયા સામેની વનડે શ્રેણી માટે ટીમ ઈન્ડિયાની જાહેરાત')]\n",
      "[('This was another topping.', 'Rehab and relapse', 'There are various options available:'), ('આ બીજો મોટો ધડાકો હતો.', 'પુનઃગઠન અને ફડચો', 'વિવિધ વૈકલ્પિક રસ્તાઓ છે:')]\n",
      "[('On the lines of last years Howdy-Modi event in Houston, US President Donald Trump will address a similar event in PM Modis home state of Gujarat when he visits India in the last week of February.', 'The results are as follow:', 'His father was a lecturer in physics and mathematics.'), ('આગામી ફેબ્રુઆરીમાં ડોનાલ્ડ ટ્રમ્પની સંભવિત ભારત યાત્રા વખતે વડાપ્રધાન મોદીનાં હોમ સ્ટેટ ગુજરાતનાં સૌથી મોટા શહેર અમદાવાદ ખાતે હાઉડી મોદી જેવો કાર્યક્રમ યોજાય અને તેમાં વડાપ્રધાન મોદી પોતે હાજર રહે તેવી શક્યતા છે.', 'પરિણામ નીચે પ્રમાણે છે.', 'તેમના પિતા ગણિત અને ભૌતિકશાસ્ત્રના પ્રોફેસર હતાં.')]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)\n",
    "\n",
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, language_to_index, start_token=True, end_token=True):\n",
    "    sentence_word_indicies = [language_to_index[token] for token in list(sentence)]\n",
    "    if start_token:\n",
    "        sentence_word_indicies.insert(0, language_to_index[START_TOKEN])\n",
    "    if end_token:\n",
    "        sentence_word_indicies.append(language_to_index[END_TOKEN])\n",
    "    for _ in range(len(sentence_word_indicies), max_sequence_length):\n",
    "        sentence_word_indicies.append(language_to_index[PADDING_TOKEN])\n",
    "    return torch.tensor(sentence_word_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenized, gu_tokenized = [], []\n",
    "for sentence_num in range(batch_size):\n",
    "    eng_sentence, gu_sentence = batch[0][sentence_num], batch[1][sentence_num]\n",
    "    eng_tokenized.append( tokenize(eng_sentence, english_to_index, start_token=False, end_token=False) )\n",
    "    gu_tokenized.append( tokenize(gu_sentence, gujarati_to_index, start_token=True, end_token=True) )\n",
    "eng_tokenized = torch.stack(eng_tokenized)\n",
    "gu_tokenized = torch.stack(gu_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
